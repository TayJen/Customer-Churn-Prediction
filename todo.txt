Done:
    - Check the kmeans+svm+smote model for overfitting
        Answer: There are no overfitting, in order to verify additional test split was made
    - Finish SHAP for kmeans-svm-smote
        Answer: There are different issues with using SHAP with SVM, so we will catch it later
    - Try other oversampling techniques (first read everything in three last articles)
        Answer: 
    - Try logistic regression with best sampling technique
    -- MLFlow
    - Different feature selection techniques
    - Feature selection based on SHAP conclusions
    - Dealing with dependencies (`pipreqs` and `pipreqsnb` helped me, also it is important to have full path without spaces,
        it will help you a lot in the future)

More details:
    - Read about Catboost
    - Read about Lightgbm (what's the difference between other gbms?)
    - Create different experiment for different gbm models amd compare them (XGB, Lightgbm, Catboost)
    - Try to tune all of them and compare results using MLFLOW

Read and try:
    - Hyperparameter tuning (with MLFLOW?)
    - Other models such as Lightgbm, Catboost?
    - Putting everything into Docker

