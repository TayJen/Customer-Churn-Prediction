Done:
    - Check the kmeans+svm+smote model for overfitting
        Answer: There are no overfitting, in order to verify additional test split was made
    - Finish SHAP for kmeans-svm-smote
        Answer: There are different issues with using SHAP with SVM, so we will catch it later
    - Try other oversampling techniques (first read everything in three last articles)
        Answer: 
    - Try logistic regression with best sampling technique
    -- MLFlow
    - Create different experiment for different gbm models amd compare them (XGB, Lightgbm, Catboost)
    - Different feature selection techniques
    - Feature selection based on SHAP conclusions
    - Dealing with dependencies (`pipreqs` and `pipreqsnb` helped me, also it is important to have full path without spaces,
        it will help you a lot in the future)
    - Read about Catboost
    - Read about Lightgbm (what's the difference between other gbms?)
    - Update search space for LightGBM (read about it in resources.txt)
    - Create experiments with LightGBM
    - Try to tune all of them and compare results using MLFLOW

To Do:
    - Create beautiful Readme
    - By comparing results of all the gbm tuned models, select one and submit to kaggle
    - Try to integrate everything you've done to readme, including mlflow tracking
    - Update requirements.txt